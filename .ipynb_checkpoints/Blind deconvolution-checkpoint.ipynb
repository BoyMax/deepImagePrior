{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "import numpy as np\n",
    "from models import *\n",
    "\n",
    "import torch\n",
    "import torch.optim\n",
    "\n",
    "from skimage.measure import compare_psnr\n",
    "from models.blindconvolution import BiConvolution\n",
    "\n",
    "from utils.sr_utils import *\n",
    "\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "dtype = torch.cuda.FloatTensor\n",
    "#dtype = torch.FloatTensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 9, 9)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Starts here\n",
    "#path_LR_image = 'data/sr/images/Cameraman256_gauss.png'\n",
    "#path_HR_image = 'data/sr/images/Cameraman256.png'\n",
    "kernel_path ='data/sr/images/kernel_gauss.png'\n",
    "img_k_pil, img_k_np = get_image(kernel_path, -1)\n",
    "#img_lr_pil, img_lr_np = get_image(path_LR_image, -1)\n",
    "#img_hr_pil, img_hr_np = get_image(path_HR_image, -1)\n",
    "#plot_image_grid([img_lr_np,img_hr_np])\n",
    "img_k_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_depth = 32\n",
    " \n",
    "INPUT =     'noise'\n",
    "pad   =     'reflection'\n",
    "OPT_OVER =  'net'\n",
    "KERNEL_TYPE='udf'\n",
    "\n",
    "LR = 0.01\n",
    "tv_weight = 0.0\n",
    "num_iter=10000\n",
    "reg_noise_std = 0.03\n",
    "\n",
    "\n",
    "OPTIMIZER = 'adam'\n",
    "\n",
    "PLOT = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nnet_input = get_noise(input_depth, INPUT, (img_hr_pil.size[1], img_hr_pil.size[0])).type(dtype).detach()\\nnet_input_kernel= get_noise(input_depth, INPUT, (img_k_pil.size[1], img_k_pil.size[0])).type(dtype).detach()\\n\\n\\nNET_TYPE = 'skip' # UNet, ResNet\\nnet = get_net(input_depth, 'skip', pad, n_channels=1, skip_n33d=128, skip_n33u=128, skip_n11=4, \\n              num_scales=5, upsample_mode='bilinear').type(dtype)\\nnet_kernel = biget_net(input_depth, 'skip', pad, n_channels=1, skip_n33d=128, skip_n33u=128, skip_n11=4, \\n              num_scales=5, upsample_mode='bilinear').type(dtype)\\n\\n#out_kernel = net_kernel(net_input_kernel)\\n#out_kernel = torch.nn.Softmax(2)(out_kernel.view(*out_kernel.size()[:2], -1)).view_as(out_kernel)\\n#out_kernel = out_kernel.squeeze(dim=1)\\n#out_kernel = out_kernel.squeeze(dim=0)\\n\\n#out_kernel_np = out_kernel.detach().numpy()\\n# Losses\\nmse = torch.nn.MSELoss().type(dtype)\\nimg_LR_var = np_to_torch(img_lr_np).type(dtype)\\nimg_k_var = np_to_torch(img_k_np).type(dtype)\\n#biconvolution = BiConvolution(n_planes=1, kernel_type=KERNEL_TYPE, kernel_path=kernel_path,kernel=out_kernel,preserve_size=True).type(dtype)\\n#out_HR = net(net_input)\\n#out_LR = biconvolution(out_HR)\\n\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "net_input = get_noise(input_depth, INPUT, (img_hr_pil.size[1], img_hr_pil.size[0])).type(dtype).detach()\n",
    "net_input_kernel= get_noise(input_depth, INPUT, (img_k_pil.size[1], img_k_pil.size[0])).type(dtype).detach()\n",
    "\n",
    "\n",
    "NET_TYPE = 'skip' # UNet, ResNet\n",
    "net = get_net(input_depth, 'skip', pad, n_channels=1, skip_n33d=128, skip_n33u=128, skip_n11=4, \n",
    "              num_scales=5, upsample_mode='bilinear').type(dtype)\n",
    "net_kernel = biget_net(input_depth, 'skip', pad, n_channels=1, skip_n33d=128, skip_n33u=128, skip_n11=4, \n",
    "              num_scales=5, upsample_mode='bilinear').type(dtype)\n",
    "\n",
    "#out_kernel = net_kernel(net_input_kernel)\n",
    "#out_kernel = torch.nn.Softmax(2)(out_kernel.view(*out_kernel.size()[:2], -1)).view_as(out_kernel)\n",
    "#out_kernel = out_kernel.squeeze(dim=1)\n",
    "#out_kernel = out_kernel.squeeze(dim=0)\n",
    "\n",
    "#out_kernel_np = out_kernel.detach().numpy()\n",
    "# Losses\n",
    "mse = torch.nn.MSELoss().type(dtype)\n",
    "img_LR_var = np_to_torch(img_lr_np).type(dtype)\n",
    "img_k_var = np_to_torch(img_k_np).type(dtype)\n",
    "#biconvolution = BiConvolution(n_planes=1, kernel_type=KERNEL_TYPE, kernel_path=kernel_path,kernel=out_kernel,preserve_size=True).type(dtype)\n",
    "#out_HR = net(net_input)\n",
    "#out_LR = biconvolution(out_HR)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closure():\n",
    "    global i, net_input, net_input_kernel,info_dict,X,psnr_HR_max,K,psnr_k_max\n",
    "    \n",
    "    reg_noise_std = 0.01\n",
    "    net_input = net_input_saved + (noise.normal_() * reg_noise_std)\n",
    "    net_input_kernel = net_input_kernel + (noise_kernel.normal_() * reg_noise_std)\n",
    "\n",
    "    out_HR = net(net_input)\n",
    "    out_kernel = net_kernel(net_input_kernel)\n",
    "    print(out_kernel.shape)\n",
    "    out_kernel = torch.nn.Softmax(2)(out_kernel.view(*out_kernel.size()[:2], -1)).view_as(out_kernel)\n",
    "    print(out_kernel.shape)\n",
    "    padding = nn.ReflectionPad2d(4)\n",
    "    out_LR = torch.nn.functional.conv2d(padding(out_HR), out_kernel, padding=0)\n",
    "\n",
    "    total_loss = mse(out_LR, img_LR_var)\n",
    "    \n",
    "    if tv_weight > 0:\n",
    "        total_loss += tv_weight * tv_loss(out_HR)\n",
    "        \n",
    "    total_loss.backward()\n",
    "\n",
    "    # Log\n",
    "    #print(img_lr_np.shape,torch_to_np(out_LR).shape)\n",
    "    psnr_LR = compare_psnr(img_lr_np, torch_to_np(out_LR))\n",
    "    psnr_HR = compare_psnr(img_hr_np, torch_to_np(out_HR))\n",
    "    psnr_k = compare_psnr(img_k_np, torch_to_np(out_kernel.view(1, 1, 9, 9)))\n",
    "    print ('Iteration %05d' % (i), '\\r', end='')\n",
    "                      \n",
    "    # History\n",
    "    psnr_history.append([psnr_LR, psnr_HR, psnr_k])\n",
    "    \n",
    "   # if PLOT and i % 100 == 0:\n",
    "        #out_HR_np = torch_to_np(out_HR)\n",
    "        #plot_image_grid([img_hr_np, np.clip(out_HR_np, 0, 1)], factor=13, nrow=3)\n",
    "    if psnr_HR > psnr_HR_max:\n",
    "        psnr_HR_max = psnr_HR\n",
    "        X = torch_to_np(out_HR)\n",
    "        K = torch_to_np(out_kernel)\n",
    "        psnr_k_max = psnr_k\n",
    "    \n",
    "    i += 1\n",
    "    if  i == num_iter:\n",
    "        info_dict = {'psnr_HR':psnr_HR_max,'psnr_k':psnr_k_max,'X': X,'K':K}\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting optimization with ADAM\n",
      "torch.Size([1, 1, 9, 9])\n",
      "torch.Size([1, 1, 9, 9])\n",
      "(1, 512, 512) (1, 512, 512)\n",
      "torch.Size([1, 1, 9, 9])\n",
      "torch.Size([1, 1, 9, 9])\n",
      "(1, 512, 512) (1, 512, 512)\n",
      "torch.Size([1, 1, 9, 9])\n",
      "torch.Size([1, 1, 9, 9])\n",
      "(1, 512, 512) (1, 512, 512)\n",
      "torch.Size([1, 1, 9, 9])\n",
      "torch.Size([1, 1, 9, 9])\n",
      "(1, 512, 512) (1, 512, 512)\n",
      "torch.Size([1, 1, 9, 9])\n",
      "torch.Size([1, 1, 9, 9])\n",
      "(1, 512, 512) (1, 512, 512)\n",
      "torch.Size([1, 1, 9, 9])\n",
      "torch.Size([1, 1, 9, 9])\n",
      "(1, 512, 512) (1, 512, 512)\n",
      "torch.Size([1, 1, 9, 9])\n",
      "torch.Size([1, 1, 9, 9])\n",
      "(1, 512, 512) (1, 512, 512)\n",
      "torch.Size([1, 1, 9, 9])\n",
      "torch.Size([1, 1, 9, 9])\n",
      "(1, 512, 512) (1, 512, 512)\n",
      "torch.Size([1, 1, 9, 9])\n",
      "torch.Size([1, 1, 9, 9])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-2738bcd2bbfc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mpsnr_HR_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m# call network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOPTIMIZER\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m# save result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deepImagePrior/utils/common_utils.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(optimizer_type, parameters, closure, LR, num_iter)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-40-2aefdf8f507c>\u001b[0m in \u001b[0;36mclosure\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mout_LR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_HR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_kernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_LR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_LR_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtv_weight\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dip/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dip/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dip/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2154\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2155\u001b[0m         \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2156\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2157\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "result = dict()\n",
    "for photo in ['boat512','house256','Lena512','Cameraman256','image_House256rgb','image_Peppers512rgb']:\n",
    "    photo_dict = dict()\n",
    "    path_HR_image = 'data/sr/images/' + photo +'.png'\n",
    "    for kernel in ['motion']:#'gauss','motion','defocus'\n",
    "        if kernel == 'gauss':\n",
    "            kernel_path ='data/sr/kernels/kernel_gauss.mat'\n",
    "            path_LR_image = 'data/sr/images/'+ photo +'_gauss.png'\n",
    "        elif kernel == 'motion':\n",
    "            kernel_path ='data/sr/kernels/kernel_motionblur.mat'\n",
    "            path_LR_image = 'data/sr/images/'+ photo +'_motion.png'\n",
    "        elif kernel == 'defocus':\n",
    "            kernel_path ='data/sr/kernels/kernel_defocus.mat'\n",
    "            path_LR_image = 'data/sr/images/'+ photo +'_defocus.png'\n",
    "        else:\n",
    "            print('no image in kernel.')\n",
    "        \n",
    "        #continue when there is no file:\n",
    "        if os.path.isfile(path_LR_image) == False:\n",
    "            continue\n",
    "        ## load images\n",
    "        img_lr_pil, img_lr_np = get_image(path_LR_image, -1)\n",
    "        img_hr_pil, img_hr_np = get_image(path_HR_image, -1)\n",
    "        n_channels = img_hr_np.shape[0]\n",
    "        \n",
    "        net_input = get_noise(input_depth, INPUT, (img_hr_pil.size[1], img_hr_pil.size[0])).type(dtype).detach()\n",
    "        net_input_kernel= get_noise(input_depth, INPUT, (img_k_pil.size[1], img_k_pil.size[0])).type(dtype).detach()\n",
    "\n",
    "\n",
    "        NET_TYPE = 'skip' # UNet, ResNet\n",
    "        net = get_net(input_depth, 'skip', pad, n_channels=1, skip_n33d=128, skip_n33u=128, skip_n11=4, \n",
    "              num_scales=5, upsample_mode='bilinear').type(dtype)\n",
    "        net_kernel = biget_net(input_depth, 'skip', pad, n_channels=1, skip_n33d=128, skip_n33u=128, skip_n11=4, \n",
    "              num_scales=5, upsample_mode='bilinear').type(dtype)\n",
    "        mse = torch.nn.MSELoss().type(dtype)\n",
    "        img_LR_var = np_to_torch(img_lr_np).type(dtype)\n",
    "        img_k_var = np_to_torch(img_k_np).type(dtype)\n",
    "             \n",
    "        psnr_history = [] \n",
    "        net_input_saved = net_input.detach().clone()\n",
    "        net_input_kernel_saved = net_input_kernel.detach().clone()\n",
    "        noise = net_input.detach().clone()\n",
    "        noise_kernel = net_input_kernel.clone()\n",
    "\n",
    "        i = 0\n",
    "        p = get_params(OPT_OVER, net, net_input) + get_params(OPT_OVER, net_kernel, net_input_kernel)\n",
    "        mse_arr = np.zeros(num_iter)\n",
    "        info_dict = dict()\n",
    "        X = 0\n",
    "        K = 0\n",
    "        psnr_HR_max = 0\n",
    "        # call network\n",
    "        optimize(OPTIMIZER, p, closure, LR, num_iter)\n",
    "        \n",
    "        # save result\n",
    "        kernel_dict = dict()\n",
    "        kernel_dict['info'] = info_dict\n",
    "        photo_dict[kernel] = kernel_dict\n",
    "        result[photo]=photo_dict\n",
    "        np.save('result_blind'+photo+kernel+'.npy',result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "np.save('result_blind.npy',result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[6.9998e-07, 2.7926e-06, 2.9305e-05, 2.4067e-04, 6.9458e-04,\n",
       "           4.6127e-04, 1.0961e-04, 5.8597e-05, 4.2463e-05],\n",
       "          [1.1653e-04, 1.1556e-04, 1.5314e-04, 2.9397e-04, 4.1299e-04,\n",
       "           4.4591e-04, 2.8441e-04, 2.1199e-04, 2.1502e-04],\n",
       "          [2.8351e-04, 6.6404e-04, 4.9198e-03, 1.8033e-02, 1.7466e-02,\n",
       "           1.3970e-02, 8.5330e-04, 1.9501e-04, 1.0855e-04],\n",
       "          [3.6607e-04, 6.2246e-04, 8.8334e-03, 7.8323e-02, 2.5107e-02,\n",
       "           4.7272e-03, 2.1111e-03, 8.5190e-04, 8.1335e-04],\n",
       "          [3.3847e-02, 3.3322e-02, 4.0501e-02, 4.1518e-02, 5.2468e-02,\n",
       "           8.4852e-02, 1.3421e-03, 1.0177e-04, 7.1695e-05],\n",
       "          [1.0153e-04, 3.6149e-04, 1.7615e-02, 9.6808e-02, 6.7177e-02,\n",
       "           4.2900e-02, 3.0395e-02, 6.7919e-03, 2.7294e-03],\n",
       "          [2.3896e-04, 3.3711e-04, 4.5408e-04, 2.1916e-03, 6.3538e-02,\n",
       "           6.3464e-05, 8.2409e-06, 2.5290e-06, 2.9067e-06],\n",
       "          [3.6023e-05, 1.5167e-04, 1.0006e-02, 9.0557e-02, 4.8593e-02,\n",
       "           3.2152e-02, 2.9507e-03, 4.1565e-04, 2.4227e-04],\n",
       "          [1.0764e-04, 1.5899e-04, 4.3710e-04, 4.5666e-03, 7.7216e-03,\n",
       "           2.3568e-05, 2.6896e-06, 4.2846e-07, 4.0301e-07]]]],\n",
       "       grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_kernel = net_kernel(net_input_kernel)\n",
    "out_kernel = torch.nn.Softmax(2)(out_kernel.view(*out_kernel.size()[:2], -1)).view_as(out_kernel)\n",
    "out_kernel"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
